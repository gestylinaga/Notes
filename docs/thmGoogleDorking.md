# Google Dorking

## Crawlers

**Crawlers** discover content on the internet through various means. One way is by pure
discovery, where a URL is visited by the *crawler*, and information regarding the website
is returned to the search engine.

Once a crawler discovers a domain, it will index the entire contents of the domain,
looking for *keywords* and other miscellaneous information, and stores them 
into a *dictionary*.

Another method of discovery is by following any URLs found on the original website, 
and any information found on the new website is also sent to the search engine. 
Crawlers are much like a virus in that they want to *traverse*, or spread to everything 
they can. Termed as **crawling**, they traverse every URL and file they can find.

Questions:
* Name the key term of what a "crawler" is used to do:
    - `index`

* What is the name of the technique that "Search Engines" use to retrieve this info about websites?
    - `crawling`

* What is an example of the type of contents that could be gathered from a website? 
    - `keywords`


## Search Engine Optimisation


## Robots.txt


## Sitemaps


## Google Dorking


---
[back to TryHackMe main page](thm.md)

[back to Index/Table of Contents](index.md)
